{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"979686bd-5071-4700-be2f-667172613bd2","_uuid":"f4006692-20cb-4090-82f1-471edca7759c","collapsed":false,"execution":{"iopub.execute_input":"2023-05-22T14:32:41.327646Z","iopub.status.busy":"2023-05-22T14:32:41.326512Z","iopub.status.idle":"2023-05-22T14:32:41.767795Z","shell.execute_reply":"2023-05-22T14:32:41.766282Z","shell.execute_reply.started":"2023-05-22T14:32:41.327593Z"},"id":"phxlqdP6w2ha","jupyter":{"outputs_hidden":false},"outputId":"3d7779ee-c98a-48f1-8e47-e32f7ac000c3","trusted":true},"outputs":[],"source":["# for colab\n","from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"ed008418-ea84-4e73-a06d-888d5c81d14a","_uuid":"987bcdeb-95b9-4b10-a0f2-a6b4f6c16198","collapsed":false,"execution":{"iopub.execute_input":"2023-05-23T16:35:17.873497Z","iopub.status.busy":"2023-05-23T16:35:17.873122Z","iopub.status.idle":"2023-05-23T16:35:18.030688Z","shell.execute_reply":"2023-05-23T16:35:18.029765Z","shell.execute_reply.started":"2023-05-23T16:35:17.873465Z"},"id":"v2NImTxGwrEn","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import cv2\n","import os\n","import shutil"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"1bb24f0e-e649-441e-91f8-bf8d9ebd8dc8","_uuid":"fee14fd1-f12e-427d-9b85-3935fd3def5a","collapsed":false,"execution":{"iopub.execute_input":"2023-05-23T16:35:18.212583Z","iopub.status.busy":"2023-05-23T16:35:18.211197Z","iopub.status.idle":"2023-05-23T16:35:19.195982Z","shell.execute_reply":"2023-05-23T16:35:19.194861Z","shell.execute_reply.started":"2023-05-23T16:35:18.212540Z"},"id":"ktBIbHD_wrEp","jupyter":{"outputs_hidden":false},"outputId":"791205a0-93ba-4df1-e75d-b00de09a0cdf","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["rm: cannot remove './dataset': No such file or directory\n"]}],"source":["!rm -r ./dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"fdf8e4c1-03ac-4cd4-bad2-67317798b655","_uuid":"fe9af3f1-0141-42e2-9788-e195919cc7c9","collapsed":false,"execution":{"iopub.execute_input":"2023-05-23T16:35:19.199012Z","iopub.status.busy":"2023-05-23T16:35:19.198643Z","iopub.status.idle":"2023-05-23T16:35:20.141933Z","shell.execute_reply":"2023-05-23T16:35:20.140710Z","shell.execute_reply.started":"2023-05-23T16:35:19.198976Z"},"id":"NNx9ULoBxH5F","jupyter":{"outputs_hidden":false},"outputId":"3a0ee2b4-4a46-463d-a0cd-2cdec8128167","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["unzip:  cannot find or open /content/drive/MyDrive/Practicum_Autonomous/track1data.zip, /content/drive/MyDrive/Practicum_Autonomous/track1data.zip.zip or /content/drive/MyDrive/Practicum_Autonomous/track1data.zip.ZIP.\n"]}],"source":["# only for google colab\n","# unzip dataset from google drive\n","!unzip /content/drive/MyDrive/Practicum_Autonomous/track1data.zip -d dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd3015e8-54e8-41d1-99e9-1c4d36d5139f","_uuid":"285e3bc7-f422-40db-9c6e-d53f6a35e7c9","collapsed":false,"execution":{"iopub.execute_input":"2023-05-23T16:35:20.145343Z","iopub.status.busy":"2023-05-23T16:35:20.144915Z"},"id":"wuUmkyPVwrEq","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# only for kaggle\n","# copy from track1data from Data folder to /kaggle/working path\n","src = \"/kaggle/input/selfdriving-car-simulator/track1data/track1data\"\n","dest = \"./dataset\"\n","\n","shutil.copytree(src, dest)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cb48440b-4ffa-46e6-a5b4-4341323e506d","_uuid":"6d5ed3c8-7570-4395-8a51-a3f6be730c36","collapsed":false,"id":"ynaYf_yhttCZ","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# path_csv = \"/content/dataset/track1data/track1data/driving_log.csv\"\n","# path_img = \"/content/dataset/track1data/track1data/IMG/\"\n","\n","# kaggle\n","path_csv = \"/kaggle/working/dataset/driving_log.csv\"\n","path_img = \"/kaggle/working/dataset/IMG/\""]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d5289494-d49f-49a4-ab57-aaabc88c1598","_uuid":"3a062783-7c99-44cb-b2ae-9667af85e077","collapsed":false,"id":"kNRsIqICwrEq","jupyter":{"outputs_hidden":false},"outputId":"04ac3427-8c0e-4566-a289-473ba52579d8","trusted":true},"outputs":[],"source":["train_df = pd.read_csv(path_csv,\n","                       names=[\"center_cam\", \"left_cam\", \"right_cam\", \"steering_angle\", \"throttle\", \"reverse\", \"speed\"])\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da8a5c3b-d6ad-44ea-91bb-e477ec4e8dd9","_uuid":"358db94d-bd12-4e7b-be26-3fceedd9d6e0","collapsed":false,"id":"KbC7TRj2wrEr","jupyter":{"outputs_hidden":false},"outputId":"6e4961ec-43ef-4a54-ec22-3502e74d40a5","trusted":true},"outputs":[],"source":["len(train_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0d6c10f-4ff3-4aa3-ac4c-4af09a09bc3a","_uuid":"f35a67f4-8129-48bd-8330-a6743fe6607d","collapsed":false,"id":"_enMxRXQwrEr","jupyter":{"outputs_hidden":false},"outputId":"a85aaed1-6362-4ff0-a3c8-33c2f46b43a7","trusted":true},"outputs":[],"source":["train_df[\"center_cam\"] = train_df[\"center_cam\"].apply(lambda x: x.split(\"\\\\\")[-1])\n","train_df[\"left_cam\"] = train_df[\"left_cam\"].apply(lambda x: x.split(\"\\\\\")[-1])\n","train_df[\"right_cam\"] = train_df[\"right_cam\"].apply(lambda x: x.split(\"\\\\\")[-1])\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39d2de79-8332-4f09-8512-93cf071f67e3","_uuid":"4429e3bb-7068-4990-887d-64e35119ec4c","collapsed":false,"id":"I5zYtmlGwrEr","jupyter":{"outputs_hidden":false},"outputId":"c5188db0-6de8-4fc0-e8f4-1b2998e7596f","trusted":true},"outputs":[],"source":["train_df[\"center_cam\"] = path_img + train_df[\"center_cam\"]\n","train_df[\"left_cam\"] = path_img + train_df[\"left_cam\"]\n","train_df[\"right_cam\"] = path_img + train_df[\"right_cam\"]\n","train_df['center_cam'].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dac9d0a1-a66c-4a1e-ab2b-dcb66a2c25b6","_uuid":"9702d164-4d04-4702-a14c-9b7dba613c44","collapsed":false,"id":"PmMBUD38wrEs","jupyter":{"outputs_hidden":false},"outputId":"6d011e23-8c08-4674-e928-84026e9d29ed","trusted":true},"outputs":[],"source":["plt.hist(train_df.steering_angle.values, bins=20);"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77a1dd71-07fc-4b36-81ea-db2936860e72","_uuid":"c9418ffe-0416-413a-8fe5-8ee60df7dcd9","collapsed":false,"id":"ZRlL9L53wrEs","jupyter":{"outputs_hidden":false},"outputId":"afa5026a-e2b2-462a-d944-8ffd6c3d5a51","trusted":true},"outputs":[],"source":["train_df.steering_angle.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"404acd8d-eb9f-4087-b188-f72c8714d79e","_uuid":"3b9b8158-1613-484c-86ca-1a61726dbba6","collapsed":false,"id":"uUWIjJHUwrEs","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def augment_and_save(img_dir, img_names, label):\n","    new_imgs = []\n","    for img_name in img_names:\n","        img_path = img_name\n","        img = plt.imread(img_path)\n","        new_img_name = img_name.replace(\".jpg\", \"\") + \"_flipped.jpg\"\n","        new_img_path = os.path.join(img_dir, new_img_name)\n","        cv2.imwrite(new_img_path, cv2.flip(img, 1))\n","        new_imgs.append((new_img_name, (-label)))\n","    return new_imgs"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9a017948-bf81-4c77-9ff3-9632b24a5f41","_uuid":"8223c857-a161-4c73-a1ab-fefbac2c7a6a","collapsed":false,"id":"9HDOwBC5wrEs","jupyter":{"outputs_hidden":false},"outputId":"3118effc-6b05-43e6-b368-cbda968dca0d","trusted":true},"outputs":[],"source":["non_zero_df = train_df[train_df[\"steering_angle\"] != 0.0]\n","non_zero_df"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27fa153f-dc51-4afc-9bec-b4f928bcbf54","_uuid":"3fa427cb-0293-4394-845e-f1c1d9bc5be2","collapsed":false,"id":"1ZyujT5fwrEs","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["images_center = []\n","images_right = []\n","images_left = []\n","labels = []\n","\n","img_dir = path_img\n","new_imgs = []\n","for index, data in non_zero_df.iterrows():\n","    new_images = augment_and_save(img_dir, [data[\"center_cam\"], data[\"right_cam\"], data[\"left_cam\"]], data[\"steering_angle\"])\n","    images_center.append(new_images[0][0])\n","    images_right.append(new_images[1][0])\n","    images_left.append(new_images[2][0])\n","    labels.append(new_images[0][1])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"952bcf83-5c62-4cd0-b8d2-435b6a7ac824","_uuid":"88862ca7-7d57-4de3-b19c-c1710a141c23","collapsed":false,"id":"-gxHhrDywrEt","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["augmented_df = pd.DataFrame(list(zip(images_center,images_right, images_left, labels)), columns=[\"center_cam\", \"left_cam\",\"right_cam\",\"steering_angle\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9fa97dd3-fc3a-44e9-a2e5-a59a32f88d96","_uuid":"80adf518-3fc4-4ea6-9ef4-c7b970916eb9","collapsed":false,"id":"90bplrhywrEt","jupyter":{"outputs_hidden":false},"outputId":"e1f75b26-553b-4ac3-8310-119710d1e633","trusted":true},"outputs":[],"source":["augmented_df"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"48e86001-7d62-49d0-9a3d-d8a9ce1c6d6c","_uuid":"a5437f8d-c891-430c-823c-3b871e31f57b","collapsed":false,"id":"BiGcGuSnwrEt","jupyter":{"outputs_hidden":false},"outputId":"04d15a7d-c6b4-47ce-e984-2e902368ca99","trusted":true},"outputs":[],"source":["zero_df = train_df.query(\"steering_angle == 0.0\").sample(frac=.1)\n","len(zero_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df478cca-8b00-4b94-befe-401f61d09b2c","_uuid":"41fda5b1-1267-4868-9a1a-0686f6d97471","collapsed":false,"id":"jwJ1I-CjwrEt","jupyter":{"outputs_hidden":false},"outputId":"baa1d5d9-c196-497b-c6d5-5c26180603bd","trusted":true},"outputs":[],"source":["new_train_df = pd.concat([zero_df, augmented_df, non_zero_df])\n","new_train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ffb0eda4-192b-4375-985b-4fd487f6e0b9","_uuid":"b80e7039-1d3b-4eb4-98c9-dc4b95d01290","collapsed":false,"id":"r_bCOf1kwrEt","jupyter":{"outputs_hidden":false},"outputId":"3b503fde-2ea0-420b-e2b8-b86c09eaf62e","trusted":true},"outputs":[],"source":["plt.ylim(0, 300)\n","plt.hist(new_train_df.steering_angle.values, bins=50);"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b5dafc5-8636-4c8d-b6e6-2e1fa733499a","_uuid":"733b525f-c340-4ceb-882d-c5f61fce7508","collapsed":false,"id":"1m-GDbg1wrEt","jupyter":{"outputs_hidden":false},"outputId":"d3071d92-948c-4992-b77a-27b885d954e3","trusted":true},"outputs":[],"source":["new_train_df.steering_angle.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a34fd71d-079c-4b4f-a171-63e0782364f3","_uuid":"cc104be6-b959-4978-ba8f-a7d37a07f6e5","collapsed":false,"id":"CLQNSICMwrEt","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# N=200\n","# new_train_df.drop(new_train_df[new_train_df['steering_angle'] < 0.10].head(N).index, inplace=True)\n","# new_train_df.steering_angle.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4e217f8b-42eb-4182-8427-18f9ee3d54dd","_uuid":"3b3b38b7-e489-4724-92dd-56f192ae5f9e","id":"hsxWmp2zwrEt","outputId":"d4c65cd5-3355-4422-ecb9-3d08188bf095","trusted":true},"outputs":[],"source":["# plt.ylim(0, 300)\n","# plt.hist(new_train_df.steering_angle.values, bins=50);"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2883a7f0-d717-46b8-956f-a790b12e0174","_uuid":"32015c35-2296-4927-b656-6c68389b117a","collapsed":false,"id":"K3g3v0tKwrEt","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["new_train_df.to_csv(path_csv, index=False, header=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a77e17f5-2181-46ae-8e49-9c00f59fe860","_uuid":"a36f6fe8-02e5-46d0-8f0b-a3174e033ec9","collapsed":false,"id":"kEMlWnTpwrEt","jupyter":{"outputs_hidden":false},"outputId":"f08baf01-4ec9-47bd-f66b-dab5ef8244ea","trusted":true},"outputs":[],"source":["# every record has 3 images -> available images: 3*len(new_train_df)\n","len(new_train_df)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"6481a30f-d704-4001-83c3-a3c6f4b08c88","_uuid":"aa94c4f6-8dee-411d-8a08-e1be7abeabf9","id":"-dqWfBAjwrEu","trusted":true},"source":["### Using Resnet50 \n","https://arxiv.org/pdf/1912.05440.pdf -> also: worth checking the **3.1**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3362e57f-d173-4493-ad78-b2d154c86f33","_uuid":"a1b1d485-0119-4268-93b7-78c71faf4756","collapsed":false,"id":"LJ5VDhQkwrEu","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import torchvision.models as models\n","import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","\n","class TunedResnet50(nn.Module):\n","    \"\"\"\n","    * @brief Initializes the class varaibles\n","    * @param None.\n","    * @return None.\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        self.resnet50 = models.resnet50(weights=\"IMAGENET1K_V1\")\n","        self.resnet50.fc = nn.Sequential(\n","            nn.Dropout(p=0.5),\n","            nn.Linear(2048,512),\n","            nn.ELU(),\n","            nn.Dropout(p=0.5),\n","            nn.Linear(512, 256),\n","            nn.ELU(),\n","            nn.Dropout(p=0.5),\n","            nn.Linear(256, 64),\n","            nn.ELU(),\n","            nn.Dropout(p=0.5),\n","            nn.Linear(64, 1),\n","            nn.ELU(),\n","        )\n","    \"\"\" \n","    * @brief Function to build the model.\n","    * @parma The image to train.\n","    * @return The trained prediction network.\n","    \"\"\"\n","    def forward(self, input):\n","        input = self.resnet50(input)\n","        return input\n","    \n","    def get_fc_layers(self,):\n","        return self.resnet50.fc.parameters()\n","    \n","    def get_main_layers(self,):\n","        return [param for name, param in self.resnet50.named_parameters() if 'fc' not in name]"]},{"cell_type":"markdown","metadata":{},"source":["### Nvidia -> https://arxiv.org/pdf/1604.07316.pdf"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"967cbf55-8d88-434c-b665-b394b91e3bd1","_uuid":"3fcbe469-6dee-440b-b7cd-e8d0653a15c6","collapsed":false,"id":"ceoLLtE0TUks","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import torchvision.models as models\n","import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","\n","class NvidiaModel(nn.Module):\n","    \"\"\"\n","    * @brief Initializes the class varaibles\n","    * @param None.\n","    * @return None.\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        self.nvidia = nn.Sequential(\n","            nn.Conv2d(3, 24, 5, stride=2),\n","            nn.ELU(),\n","            nn.Conv2d(24, 36, 5, stride=2),\n","            nn.ELU(),\n","            nn.Conv2d(36, 48, 5, stride=2),\n","            nn.ELU(),\n","            nn.Conv2d(48, 64, 3),\n","            nn.ELU(),\n","            nn.Conv2d(64, 64, 3),\n","            nn.Dropout(0.5),\n","            nn.Flatten(),\n","            nn.Linear(1152, 100),\n","            nn.ELU(),\n","            nn.Linear(100, 50),\n","            nn.ELU(),\n","            nn.Linear(50, 10),\n","            nn.ELU(),\n","            nn.Linear(10, 1),\n","        )\n","    \"\"\" \n","    * @brief Function to build the model.\n","    * @parma The image to train.\n","    * @return The trained prediction network.\n","    \"\"\"\n","    def forward(self, input):\n","        input = self.nvidia(input)\n","        return input"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f41d21dd-51dc-43f6-b7a7-85544fba7e77","_uuid":"57a2391c-93ab-43f5-a40c-73fce6cb4309","collapsed":false,"id":"GP4AMiT7wrEv","jupyter":{"outputs_hidden":false},"outputId":"5ff8ea22-1d96-4e31-d19d-c66c0173c498","trusted":true},"outputs":[],"source":["!pip install torchsummary"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91958a06-310d-43ca-a826-04dd05439126","_uuid":"a90762ab-ca7a-4d99-b1ba-3345fec378fc","collapsed":false,"id":"EPb1uNvkwrEv","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c99ac416-d1aa-4caf-a8a0-5884dc8d077d","_uuid":"64da55db-a0d1-4b94-8730-07a3ba9481a7","collapsed":false,"id":"YzWR4Wc9wrEv","jupyter":{"outputs_hidden":false},"outputId":"c9bee5a0-470b-4ea2-fff2-a1e1fa2352fc","trusted":true},"outputs":[],"source":["from torchsummary import summary\n"," \n","model = TunedResnet50()\n","# model = NvidiaModel()\n","\n","model = model.to(device)\n","\n","summary(model, (3, 224, 224))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc8d0409-7f8a-4e32-951a-9f1dbd51cc21","_uuid":"e63c216c-abc4-4af9-b16d-586d68afd20e","collapsed":false,"id":"hgAvHo_kwrEv","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import csv\n","import os\n","from torch.utils.data import Dataset\n","import cv2\n","\n","\n","\"\"\"\n"," * @brief The class Features is used to read and seperate various parameters from the \n"," * csv file and return them as arrays for the train process.\n"," * @ param Gets the dataset directory path.\n"," \"\"\"\n","class Features(Dataset):\n","    \"\"\"\n","    * @brief Initializes the parameters in the Features class.\n","    * @param Path to the dataset.\n","    * @return None. \n","    \"\"\"\n","    def __init__(self, path_to_csv):\n","        #imports the dataset path and joins it to the csv log file.\n","        path_to_csv = os.path.join(path_to_csv, 'driving_log.csv')\n","        self.csv_data = self.load_csv_file(path_to_csv)\n","    \"\"\"\n","    * @brief Function to load the csv file and seperate the values written in them.\n","    * @param Path to the csv file.\n","    * @return The split and extracted consolidated data.\n","    \"\"\"\n","    def load_csv_file(self, path_to_csv):\n","        data = []\n","        with open(path_to_csv, 'r') as csvfile:\n","            data_reader = csv.reader(csvfile, delimiter=',')\n","            for row in data_reader:\n","                data.append(row)\n","        return data\n","    \"\"\"\n","    * @brief Function to test the return of csv data\n","    * @param None.\n","    * @return The csv data\n","    \"\"\" \n","    def get_csv_data(self):\n","        return self.csv_data\n","    \"\"\"\n","    * @brief Function to return the length of the dataset.\n","    * @param None.\n","    * @return The length of the dataset.\n","    \"\"\"\n","    def __len__(self):\n","        return len(self.csv_data)\n","    \"\"\"\n","    * @brief Function to return one data point from the csv file.\n","    * @param The index of the data to return.\n","    * @return The data value of the index\n","    \"\"\" \n","    def __getitem__(self,i):\n","        data_entry = self.csv_data[i]\n","        # Splitting the data of the features from one single data point.\n","        to_return = {\n","            'img_center_pth': data_entry[0],\n","            'img_left_pth': data_entry[1],\n","            'img_right_pth': data_entry[2],\n","            'steering_angle': data_entry[3],\n","            'throttle': data_entry[4],\n","            'brake': data_entry[5],\n","            'speed': data_entry[6]\n","        }\n","\n","        return to_return"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c9c4a7bb-3ff2-4c1c-8998-754a03c3b87a","_uuid":"27bbad1d-6387-468b-990a-13c6d60a0c2b","collapsed":false,"id":"xzAd8YWnwrEv","jupyter":{"outputs_hidden":false},"outputId":"0d319a51-fac5-41e6-c277-7b5e350c2ddc","trusted":true},"outputs":[],"source":["!pip install tensorboard_logger"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d81c745c-ab63-461a-9748-5e2f13a5133a","_uuid":"bcc524b7-9954-4b9a-9ef7-4dac9eb1f675","collapsed":false,"id":"TYsW4GyXwrEv","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import cv2, os\n","import numpy as np\n","import matplotlib.image as mpimg\n","\n","# declaration of image parameters\n","IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 66, 200, 3\n","INPUT_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)\n","\n","\"\"\"\n","* @brief Function to load the images from the path \n","* @param data direcrtory \n","* @param File path of the images\n","* @return The image file\n","\"\"\"\n","def load_image(data_dir, image_file):\n","    \"\"\"\n","    Load RGB images from a file\n","    \"\"\"\n","    return mpimg.imread(os.path.join(data_dir, image_file.strip()))\n","\n","\"\"\"\n","* @brief Function to crop the imeages to the required shape\n","* @param The image to crop\n","* @return The cropped image\n","\"\"\"\n","def crop(image):\n","   \n","   # Crop the image (removing the sky at the top and the car front at the bottom)\n","    \n","    return image[60:-25, :, :] # remove the sky and the car front\n","\n","\"\"\"\n","* @brief Resize the image to the input shape used by the network model\n","* @param Image file to Resize\n","* @return The Resized image for the network\n","\"\"\" \n","def resize(image):\n","    return cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT), cv2.INTER_AREA)\n","\n","\"\"\"\n","* @brief Fuinction to convert the color space of the image from RGB to YUV.\n","* @param The image to change the colorspace.\n","* @return The converted image. \n","\"\"\"\n","def rgb2yuv(image):\n","    return cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n","\n","\"\"\"\n","* @brief Function to run the preprocess of the images.\n","* @param The images from the dataset\n","* @return The preprocessed images.\n","\"\"\"\n","def preprocess(image, train=True):\n","    image = crop(image)\n","    image = resize(image)\n","    if train:\n","        image = cv2.GaussianBlur(image, (3,3), 0)\n","    image = rgb2yuv(image)\n","    return image\n","\n","\"\"\"\n","* @brief Function to randomly choose the center, left and right images \n","* to adjust the steering angles.\n","* @param Steering angles that corresponds to the images.\n","* @return The new adjusted steering angles.\n","\"\"\"\n","def choose_image(steering_angle):\n","    choice = np.random.choice(3)\n","    if choice == 0:\n","        return \"img_left_pth\", float(steering_angle) + 0.2\n","    elif choice == 1:\n","        return \"img_right_pth\", float(steering_angle) - 0.2\n","    return \"img_center_pth\", float(steering_angle)\n","\n","\"\"\"\n","* @brief Function to randomly flip the left and right images and adjust\n","* the steering angles.\n","* @param The images from the left or right dataset.\n","* @param The steering angle of the corresponding images.\n","* @return the flipped images and their corresponding steering angles.\n","\"\"\"\n","def random_flip(image, steering_angle):\n","    if np.random.rand() < 0.5:\n","        image = cv2.flip(image, 1)\n","        steering_angle = -steering_angle\n","    return image, steering_angle\n","\n","\"\"\"\n","* @brief Fuinction to randomly translate the image vertically and horizontally.\n","* @param The image to translate.\n","* @param The steering angle of the selected image.\n","* @param Range of x translation.\n","* @param Range of y translation.\n","* @return The image and its corresponding steering angle\n","\"\"\"\n","def random_translate(image, steering_angle, range_x, range_y):\n","    trans_x = range_x * (np.random.rand() - 0.5)\n","    trans_y = range_y * (np.random.rand() - 0.5)\n","    steering_angle += trans_x * 0.002\n","    trans_m = np.float32([[1, 0, trans_x], [0, 1, trans_y]])\n","    height, width = image.shape[:2]\n","    image = cv2.warpAffine(image, trans_m, (width, height))\n","    return image, steering_angle\n","\n","\"\"\"\n","* @brief Randomly generates shadows in the image.\n","* @param The image to add shadow on.\n","* @return The image with added shadows.\n","\"\"\" \n","def random_shadow(image):\n","    # (x1, y1) and (x2, y2) forms a line\n","    # xm, ym gives all the locations of the image\n","    x1, y1 = IMAGE_WIDTH * np.random.rand(), 0\n","    x2, y2 = IMAGE_WIDTH * np.random.rand(), IMAGE_HEIGHT\n","    xm, ym = np.mgrid[0:IMAGE_HEIGHT, 0:IMAGE_WIDTH]\n","\n","    mask = np.zeros_like(image[:, :, 1])\n","    mask[np.where((ym - y1) * (x2 - x1) - (y2 - y1) * (xm - x1) > 0)] = 1\n","\n","    # choose which side should have shadow and adjust saturation\n","    cond = mask == np.random.randint(2)\n","    s_ratio = np.random.uniform(low=0.2, high=0.5)\n","\n","    # adjust Saturation in HLS(Hue, Light, Saturation)\n","    hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n","    hls[:, :, 1][cond] = hls[:, :, 1][cond] * s_ratio\n","    return cv2.cvtColor(hls, cv2.COLOR_HLS2RGB)\n","\n","\"\"\"\n","* @brief Function to adjust the brightness of the images.\n","* @param The image to process\n","* @return The brightness equalized image.\n","\"\"\"\n","def random_brightness(image):\n","    # HSV (Hue, Saturation, Value) is also called HSB ('B' for Brightness).\n","    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n","    ratio = 1.0 + 0.4 * (np.random.rand() - 0.5)\n","    hsv[:,:,2] =  hsv[:,:,2] * ratio\n","    return cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n","\n","\"\"\"\n","* @brief Function to augment the image and trhe steering angle of the center image\n","* @param The data directory containing thje images.\n","* @param The center, left and right images and their corresponding steering angles.\n","* @param The x and y range of the image augmentation allowd(set to a default value\n","* unless specified on execution)\n","* @return Returns the output from all image augmentation process.\n","\"\"\"\n","def augument(data_dir, center, left, right, steering_angle, range_x=100, range_y=10):\n","    image, steering_angle = choose_image(data_dir, center, left, right, steering_angle)\n","    image, steering_angle = random_flip(image, steering_angle)\n","    image, steering_angle = random_translate(image, steering_angle, range_x, range_y)\n","    image = random_shadow(image)\n","    image = random_brightness(image)\n","    return image, steering_angle\n","\n","\"\"\"\n","* @brief Function to Generate training image give image paths and associated steering angles\n","* @param The directory path where the data is stored.\n","* @param The image paths for each image.\n","* @param The steering angles for the images.\n","* @param The variable containing the size of the required batch size.\n","* @param Binary value of the status of the training process.\n","* @return The images and steering angles.\n","\"\"\"\n","def batch_generator(data_dir, image_paths, steering_angles, batch_size, is_training):\n","    images = np.empty([batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS])\n","    steers = np.empty(batch_size)\n","    while True:\n","        i = 0\n","        for index in np.random.permutation(image_paths.shape[0]):\n","            center, left, right = image_paths[index]\n","            steering_angle = steering_angles[index]\n","            # argumentation\n","            if is_training and np.random.rand() < 0.6:\n","                image, steering_angle = augument(data_dir, center, left, right, steering_angle)\n","            else:\n","                image = load_image(data_dir, center)\n","            # add the image and steering angle to the batch\n","            images[i] = preprocess(image)\n","            steers[i] = steering_angle\n","            i += 1\n","            if i == batch_size:\n","                break\n","        yield images, steers"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ff30488-2fc5-4942-90d5-c0b821f97ad5","_uuid":"c1844ba7-8ab0-4f66-a02d-969c8ff09e1c","collapsed":false,"id":"6yseppRPwrEw","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\"\"\"\n","  *  @copyright (c) 2020 Charan Karthikeyan P V, Nagireddi Jagadesh Nischal\n","  *  @file    train.py\n","  *  @author  Charan Karthikeyan P V, Nagireddi Jagadesh Nischal\n","  *\n","  *  @brief Main file to train and evaluate the model.  \n"," \"\"\"\n","import os\n","import time\n","import itertools\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from tqdm import tqdm\n","tqdm.monitor_interval = 0\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from tensorboard_logger import configure, log_value\n","from torch.utils.data.sampler import RandomSampler, SequentialSampler\n","import matplotlib.pyplot as plt\n","import argparse\n","from matplotlib import pyplot as plt\n","import cv2\n","\n","history_train_loss = []\n","history_val_loss = []\n","\"\"\"\n","* @brief Function to train the model with the input data and save them.\n","* @param The arguments containing the parameters \n","*  needed to train and generate the model.\n","* @param The model to train the data with.\n","* @param The split datatset to train.\n","* @param The validation dataset.\n","* @return None.\n","\"\"\"\n","def train_model(args, model, dataset_train, dataset_val):\n","    # Imports the training model.\n","    model.train()\n","    #Declaration of the optimizer and the loss model.\n","    # used for TunedResNet50\n","    optimizer = optim.Adam([{'params': model.get_main_layers(), 'lr': 0.00001}, {\"params\": model.get_fc_layers()}],\n","                          lr=args['learning_rate'],\n","                          weight_decay=0.0003)\n","#     optimizer = optim.Adam(model.parameters(), lr=args['learning_rate'])\n","    criterion = nn.MSELoss()\n","    imgs_per_batch = args['batch_size'] #gets the batch size from the argument parameters\n","    optimizer.zero_grad()\n","    for epoch in range(args['nb_epochs']): # runs for the number of eposchs set in the arguments\n","        sampler = RandomSampler(dataset_train)\n","        train_loss = 0.0\n","        batch_images = 0 \n","        for i, sample_id in enumerate(sampler):\n","            data = dataset_train[sample_id]\n","#             print(\"data: \", data)\n","            label = data['steering_angle'] #, data['brake'], data['speed'], data['throttle']\n","            # right, center, left images\n","            for j in range(1,4):\n","                img_pth, label = choose_image(label)\n","                # Data augmentation and processing steps\n","        #             print(\"img_pth: \", data[img_pth], \" label: \", label)\n","                img = cv2.imread(data[img_pth])\n","\n","                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","                img = preprocess(img)\n","                img, label = random_translate(img, label, 100, 10)\n","                img = random_shadow(img)\n","                img = random_brightness(img)\n","                img = Variable(torch.cuda.FloatTensor([img]))\n","                label = np.array([label]).astype(float)\n","                label = Variable(torch.cuda.FloatTensor(label))\n","                img = img.permute(0,3,1,2)\n","                img = img/255\n","\n","                #training and loss calculation\n","                out_vec = model(img)\n","                loss = criterion(torch.squeeze(out_vec, dim=0),label)\n","                train_loss += loss.item()\n","\n","                loss.backward()\n","                batch_images +=1\n","                if batch_images%imgs_per_batch==0:\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","            \n","        # Validation of the model mid training for better understanding and visualization\n","        val_loss = eval_model(model,dataset_val)\n","#                 log_value('val_loss',val_loss,step)\n","        log_str = \\\n","            'Epoch: {} | Train Loss: {:.8f} | Val Loss: {:.8f}'\n","        log_str = log_str.format(\n","            epoch+1,\n","            train_loss/len(dataset_train),\n","            val_loss)\n","        print(log_str)\n","        model.train()  # resumes the training process\n","        \n","        history_val_loss.append(val_loss)\n","        history_train_loss.append(train_loss/len(dataset_train))\n","\n","        if (epoch+1)%5==0:\n","            # Saves the intermediate points in the training process for testing in simulator.\n","            if not os.path.exists(args['model_dir']):\n","                os.makedirs(args['model_dir'])\n","\n","            reflex_pth = os.path.join(\n","                args['model_dir'],\n","                'full_resnet_epoch{}.pth'.format(epoch+1))\n","            torch.save(\n","                model.state_dict(),\n","                reflex_pth)\n","\n","\"\"\"\n","* @brief Function to evaluate the model generated by the training process\n","* @param Model to be evaluated\n","* @param The validation dataset\n","* @param the sample size to evaluate.\n","* @return The validarion loss.\n","\"\"\" \n","def eval_model(model,dataset):\n","    model.eval()\n","    criterion = nn.MSELoss()\n","    val_loss = 0\n","    sampler = RandomSampler(dataset)\n","    torch.manual_seed(0)\n","    for i, sample_id in enumerate(sampler):\n","        data = dataset[sample_id]\n","        for j in range(1,4):\n","            img_pth, label = choose_image(data['steering_angle'])\n","            # image preprocessing and augmentation.\n","            img = cv2.imread(data[img_pth])\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","            img = preprocess(img, train=False)\n","    #         img, label = random_flip(img, label)\n","    #         img, label = random_translate(img, label, 100, 10)\n","    #         img = random_shadow(img)\n","    #         img = random_brightness(img)\n","            img = Variable(torch.cuda.FloatTensor([img]))\n","            img = img.permute(0,3,1,2)\n","            label = np.array([label]).astype(float)\n","            label = Variable(torch.cuda.FloatTensor(label))\n","            img = img / 255\n","            out_vec = model(img)\n","    #         print(out_vec.shape, \" \", label.shape)\n","            loss = criterion(torch.squeeze(out_vec, dim=0),label)\n","\n","            val_loss += loss.data.item()\n","\n","    val_loss = val_loss / len(dataset)\n","    return val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7aed87ca-7b79-4540-b078-2ded71e0b466","_uuid":"b7449f3e-5883-4379-8fa4-ce17b8410181","collapsed":false,"id":"fFAwr7hNwrEw","jupyter":{"outputs_hidden":false},"outputId":"ef13ff0d-1b2c-4657-dc3d-fe44d241ff8f","trusted":true},"outputs":[],"source":["def main(args):\n","\t#build and import the network model.\n","    # model = model_cnn()\n","    model = TunedResnet50()\n","#     model = NvidiaModel()\n","#     model = nn.DataParallel(model)\n","    #Check for cuda availability\n","    if torch.cuda.is_available():\n","        model = model.cuda()\n","\n","\n","    print('Creating model ...')\n","#     configure(\"log/\")\n","    print('Creating data loaders ...')\n","    dataset = Features(args['data_dir'])\n","    train_size = int(args['train_size'] * len(dataset))\n","    test_size = len(dataset) - train_size\n","    print(\"train size: \", 3*train_size)\n","    print(\"test size: \", 3*test_size)\n","    dataset_train, dataset_val = torch.utils.data.dataset.random_split(dataset,[train_size, test_size])\n","    \n","    train_model(args, model,dataset_train, dataset_val)\n","\n","\"\"\"\n","* @brief Runs the main function and gets the arguments from the user or \n","* takes in the default set values\n","\"\"\"\n","if __name__ == '__main__':\n","    \n","    args = {\n","        'data_dir': '/kaggle/working/dataset', # data path folder\n","        'model_dir': '/kaggle/working', # where to save the models checkpoints\n","        'train_size': 0.8,\n","        'keep_prob': 0.5, # not working\n","        'nb_epochs': 40,\n","        'samples_per_epoch': 20000, # not working\n","        'batch_size': 64,\n","        'learning_rate': 0.001,\n","    }\n","    main(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"40bc421d-bfdb-496a-93d8-061e6189f219","_uuid":"3036e821-77cf-4b73-b3e6-17a7462ecde8","collapsed":false,"id":"cryLP7TKwrEw","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["plt.plot(list(range(1,41)),history_train_loss, label=\"train_loss\")\n","plt.plot(list(range(1,41)),history_val_loss, label=\"val_loss\")\n","#     plt.savefig('train_loss(%d,%f,%f).png'%(args['nb_epochs'],args['learning_rate'],args['keep_prob']))\n","#     plt.clf()\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a055b030-fb3b-466a-9cf7-64b8b5914fd5","_uuid":"035080b6-4ed0-4767-9f71-8de6b66f39d9","collapsed":false,"id":"dxoWFVaeeICI","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# img_path = os.path.join(img_dir, images_center[0])\n","# img = plt.imread(img_path)\n","# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","# img = preprocess(img)\n","# label = labels[0]\n","# img, label = random_flip(img, label)\n","# img, label = random_translate(img, label, 100, 10)\n","# img = random_shadow(img)\n","# img = random_brightness(img)\n","# plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
